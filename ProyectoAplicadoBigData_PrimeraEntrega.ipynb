{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Robert-Gomez-AI/HeartFailure/blob/main/ProyectoAplicadoBigData_PrimeraEntrega.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=110NHJ-qD3Maf3x2GX8ngyEREwaBjKpuo\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ],
      "metadata": {
        "id": "2qrgS_sCJquS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9669dfd"
      },
      "source": [
        "# **Definición e implementación de las tecnologías**\n",
        "---\n",
        "\n",
        "Este notebook es una plantilla que le puede servir como guía para el segundo entregable del proyecto aplicado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9T83K4_xNn8"
      },
      "source": [
        "# **Integrantes**\n",
        "---\n",
        "\n",
        "- Daniela Mejia\n",
        "- Yohjan Roldan\n",
        "- Robert Gomez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6de2158"
      },
      "source": [
        "## **1. Tecnologías a utilizar**\n",
        "---\n",
        "\n",
        "Se debe proporcionar una justificación sólida para la elección de las tecnologías específicas que se utilizarán en el proyecto de *Big Data*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20031c7b"
      },
      "source": [
        "### **1.1. Selección de tecnologías**\n",
        "---\n",
        "\n",
        "- ¿Cuáles son las tecnologías seleccionadas para el proyecto?\n",
        "- ¿Por qué se eligieron estas tecnologías en lugar de otras alternativas?\n",
        "- ¿Cómo se alinean las tecnologías seleccionadas con los objetivos del proyecto?\n",
        "- ¿Cuáles son las ventajas clave de cada tecnología seleccionada?\n",
        "- ¿Existen desventajas o limitaciones que deben ser abordadas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0f9910"
      },
      "source": [
        "### **Introducción al proyecto**\n",
        "Para el proyecto  se han seleccionado un conjunto de tecnologías que se alinean con los objetivos del mismo y ofrecen ventajas clave en términos de facilidad de uso, rendimiento y flexibilidad.\n",
        "\n",
        "1.   Tecnologías seleccionadas\n",
        "* Las principales tecnologías seleccionadas para este proyecto son:\n",
        "* Lenguaje de programación: Python\n",
        "* Bibliotecas de procesamiento de lenguaje natural (NLP): NLTK, spaCy\n",
        "* Bibliotecas de aprendizaje computacional: scikit-learn, TensorFlow, PyTorch\n",
        "* Google Colab para el desarrollo colaborativo y la ejecución de código en la nube\n",
        "\n",
        "2. Beneficios de las Tecnologías Seleccionadas:\n",
        "\n",
        "  2.1. Python: Es un lenguaje de programación ampliamente utilizado en la comunidad de ciencia de datos y aprendizaje automático. Ofrece una gran cantidad de bibliotecas y una comunidad activa de desarrollo, lo que facilita la implementación de soluciones para el análisis de emociones.\n",
        "  \n",
        "  2.2. NLTK y spaCy: Estas bibliotecas de procesamiento de lenguaje natural proporcionan funcionalidades avanzadas, como tokenización, lematización y análisis gramatical, que son fundamentales para el análisis de texto y la extracción de características relevantes para el reconocimiento de emociones.\n",
        "\n",
        "  2.3. TensorFlow y PyTorch: Estos frameworks de aprendizaje profundo permiten implementar modelos complejos de análisis de sentimientos con alta eficiencia computacional. Esto es crucial para lograr una alta precisión en la clasificación de emociones en el conjunto de datos.\n",
        "\n",
        "  2.3.   Google Colab: Proporciona recursos de computación gratuitos en la nube y una interfaz colaborativa, lo que facilita el desarrollo y la experimentación del proyecto de análisis de emociones. Esto es especialmente útil cuando se trabaja con grandes volúmenes de datos y se requiere acceso a recursos de cómputo potentes.\n",
        "\n",
        "3. Alineación de las tecnologías seleccionadas con los objetivos del proyecto:\n",
        "\n",
        "Las tecnologías seleccionadas para este proyecto de análisis de emociones se alinean perfectamente con los objetivos.\n",
        "\n",
        "Python y las bibliotecas de procesamiento de lenguaje natural (NLP) como NLTK y spaCy ofrecen las herramientas necesarias para procesar y analizar el texto del conjunto de datos.\n",
        "\n",
        "Por otro lado, las bibliotecas de aprendizaje automático como TensorFlow y PyTorch permiten implementar modelos complejos de análisis de sentimientos con alta eficiencia computacional.\n",
        "\n",
        "Finalmente, Google Colab facilita el desarrollo colaborativo y el acceso a recursos de cómputo en la nube, lo que agiliza el proceso de experimentación y mejora del modelo.\n",
        "\n",
        "En resumen, el uso de estas tecnologías permite abordar de manera efectiva los objetivos del proyecto de análisis de emociones en el conjunto de datos proporcionado.\n",
        "\n",
        "4. Las tecnologías seleccionadas para el proyecto presentan ventajas significativas, pero también es importante considerar las posibles desventajas y limitaciones:\n",
        "\n",
        " * Python: Aunque es ampliamente utilizado, puede tener un rendimiento inferior en comparación con lenguajes compilados para tareas intensivas en cálculos.\n",
        " * NLTK y spaCy: Estas bibliotecas de procesamiento de lenguaje natural pueden necesitar ajustes y optimizaciones para manejar eficientemente grandes volúmenes de texto.\n",
        " * TensorFlow y PyTorch: Requieren un conocimiento profundo de conceptos de aprendizaje automático, lo que puede dificultar su configuración y entrenamiento.\n",
        " * Google Colab: A pesar de sus ventajas en el desarrollo colaborativo, puede tener restricciones en el uso de recursos computacionales y acceso a ciertas bibliotecas externas, lo que podría limitar algunas funcionalidades."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Instalación de Herramientas**\n",
        "---\n",
        "Debe describir el proceso de instalación de las herramientas y tecnologías seleccionadas e incluir los *scripts* utilizados para ello."
      ],
      "metadata": {
        "id": "qG-gA0MvzMTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ¿Cuáles son las herramientas específicas que se instalarán en el entorno? ¿Por qué?\n",
        "- ¿Existen requisitos específicos del sistema para cada herramienta?\n",
        "- ¿Cómo se configurarán y personalizarán las herramientas para adaptarse a los requisitos específicos del proyecto?\n",
        "- ¿Existen configuraciones recomendadas para optimizar el rendimiento?"
      ],
      "metadata": {
        "id": "eWhhKe7HdrKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Herramientas Específicas a Instalar:\n",
        "\n",
        "* Python\n",
        "* NLTK\n",
        "* spaCy\n",
        "* TensorFlow\n",
        "* PyTorch\n",
        "* Google Colab\n",
        "* MongoDB\n",
        "\n",
        "2. Requisitos Específicos del Sistema:\n",
        "Sistema operativo compatible con Python y las bibliotecas mencionadas.\n",
        "Espacio suficiente en disco para instalar Python y las bibliotecas.\n",
        "\n",
        "* Base de datos almacenada en MongoDB, para ello el dataset debe cumplir con los requerimientos de almacenamiento para la versión no paga de MongoDB. Es decir que preferiblemente usaremos un dataset de menos de 5GB. Sin embargo, en caso de que requiramos usar más espacio de almacenamiento o un mejor procesamiento, optaremos por contemplar otras alternativas.\n",
        "\n",
        "\n",
        "3. Configuración y Personalización:\n",
        "\n",
        "  3.1. Configurar un entorno virtual de Python para aislar las dependencias del proyecto.\n",
        "  \n",
        "  3.2. Instalar las bibliotecas utilizando pip o conda dentro del entorno virtual.\n",
        "\n",
        "  3.3. Descargar el conjunto de datos desde el URL proporcionado y almacenarlo localmente para su acceso durante el desarrollo.\n",
        "\n",
        "4. Scripts de Instalación:"
      ],
      "metadata": {
        "id": "sTGXhvF4_ROG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1: Instalar Python y herramientas básicas de desarrollo:"
      ],
      "metadata": {
        "id": "-LS7lmsE-zjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Python (puedes usar Anaconda para una instalación más completa)\n",
        "# Asegúrate de agregar Python al PATH del sistema durante la instalación."
      ],
      "metadata": {
        "id": "Zz43eObp-7UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2: Configurar un entorno virtual:"
      ],
      "metadata": {
        "id": "39razIT1-vAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un entorno virtual (ejemplo usando venv)\n",
        "\n",
        "# Activar el entorno virtual\n",
        "\n"
      ],
      "metadata": {
        "id": "m3ch-UXE-kbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3: Instalar bibliotecas necesarias:"
      ],
      "metadata": {
        "id": "PYfnOi05-_wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas de procesamiento de lenguaje natural y aprendizaje automático (spacy y scikit-learn)\n",
        "!pip install -U spacy\n",
        "!pip install nltk spacy tensorflow torch\n",
        "\n",
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "id": "P9IyB-5T_EB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47239f5-324d-4854-88c1-8114cec2f72f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4: Importar librerias necesarias"
      ],
      "metadata": {
        "id": "6vJrNGRiGZsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementaremos numpy para el uso de herramientas como arreglos y matrices multidimencionales\n",
        "import numpy as np\n",
        "#Implementaremos Scikit-learn para"
      ],
      "metadata": {
        "id": "ZfXcP6OMGdKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Configuraciones Recomendadas: Configurar el entorno virtual con las versiones específicas de las bibliotecas para garantizar la reproducibilidad del proyecto.\n",
        "Utilizar GPU si está disponible para acelerar el entrenamiento de modelos de aprendizaje profundo (requiere configuración adicional de CUDA y cuDNN para TensorFlow y PyTorch)."
      ],
      "metadata": {
        "id": "xYLHxt41_ND4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Jorge E. Camargo, PhD](https://dis.unal.edu.co/~jecamargom/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "    \n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ],
      "metadata": {
        "id": "2FLj-UiEd91C"
      }
    }
  ]
}